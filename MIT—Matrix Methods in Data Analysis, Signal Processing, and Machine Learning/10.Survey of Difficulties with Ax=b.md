# 10. Survey of Difficulties with Ax=b

## Ax=b

*easy to difficult:*

0. $x=A^+b$ , can be using in all case, **with pseudo inverse**

1. *good normal cases which has a square matrix of reasonable size and reasonable condition*$\frac{\sigma_1}{\sigma_2}$: $x=b/A$ (Elimination will be work)

2. *too many equations:* $m>n=rank$ ,  solving by least square: $A^TA\hat{x}=A^Tb$, in some cases backslash would solve the problem

3. *don't have enough equations, underdetermined and many solutions to pick one **typical of deep learning**(some many unkwon weights)* : we have to put something more to get a specific answer, pick the $min\ norm\ solution:A^+b$ or $min\ L_1\ norm\  by\ SGD$

4. *columns are nearly dependent, the inverse matrix is really big*: fix the columns: find orthonormal vectors in that column space. Two matrix connects by **gram-schmidt**: $A=QR$ in a norm way or columns pivoting (a way of reording which reording the rows while pivots are two small).

5. *matrix is nealy sigular and inverse problem*: add penlaty term $Min\ ||Ax+b||^2+\epsilon^2||x||$

   **Focus on this section**:

   *If we use elimination , it would give a poor result.*

   1. **SVD**:

   â€‹		the nearly singular matrix, it has very small singular values, but for the inverse matrix, it has very big matrix. 
   $$
   A=U\Sigma V^T \\
   A^{-1} = V \Sigma^{-1}U^T
   $$

   2. **Regularize the problem**:

      suppose that min $||Ax+b||_2^2+\delta^2||x||^2_2$, a least square with penalty , choose $\delta>0$:
      $$
      \left[ \begin{matrix} A \\ \delta I \end{matrix} \right] \left[\begin{matrix} x\end{matrix}\right] = \left[ \begin{matrix} b \\ 0 \end{matrix} \right]
      $$
      however, the choice of $\delta$ has beyond the class. It depends on the noise you want.

      With the solution of the Least Square, the form can transform to:
      $$
      (A^TA+\delta^2I)x=A^Tb
      $$
      ***Another question:*** when $\delta$ goes to zero, what happens?

      we take A as $1\times 1$ matrix to see what's going on.
      $$
      A=[\sigma]\\
      (\sigma^2 + \delta^2)x = \sigma b \rightarrow x = \frac{\sigma}{\sigma^2+\delta^2} b
      $$

      - when $\sigma > 0$:
        $$
        limit_{\delta\rightarrow0}\frac{\sigma}{\sigma^2+\delta^2} b=\frac{1}{\sigma}b
        $$

      - when $\sigma=0$:
        $$
        limit_{\delta\rightarrow0}\frac{\sigma}{\sigma^2+\delta^2} b = 0
        $$

      **So the system should choose the $\delta >0$, and when $\delta \rightarrow 0, x\rightarrow A^+b$ .**

      For any matrix :
      $$
      A=U\Sigma V^T \\
      V^TV = I\\
      U^TU=I \\
      $$
      So, the problem $(A^TA+\delta^2I)^{-1}A^T\rightarrow A^+$ can be transformed to $(\Sigma^T\Sigma+\delta^2I)^{-1}\rightarrow \Sigma^+$.

      And for  $(\Sigma^T\Sigma+\delta^2I)^{-1}\rightarrow \Sigma^+$, each matrix has been diagonal matrix, so it can be inference with above $1\times 1$ example. 

      Proof done.

      **For any matrix, $(A^TA+\delta^2I)^{-1}A^T\rightarrow A^+$= pseudoinverse when $\delta \rightarrow 0$**

      *What about L1 norm ?*

      "Lasso": Min $||Ax+b||_2^2+\delta^2||x||^2_1$ which statisticians love to use.

6. 

6. *too big*: iterative method(e.g. conjugate descent), get the answer step by step

7. *way too big that you can't see it*: randomized linear algebra, the idea is to use probability to sample the matrix and work with your samples.



