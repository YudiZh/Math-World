# Accelerating Gradient Descent 

$f(x,y)=\frac{1}{2}(x^2+by^2)$, starting from $(x_0,y_0)=(b,1)$ , we find these points:
$$
\begin{eqnarray}
x_k&=&b(\frac{b-1}{b+1})^k \\
y_k&=&(\frac{1-b}{1+b})^k \\
f(x_k,y_k)&=&(\frac{1-b}{1+b})^{2k}f(x_0,y_0)
\end{eqnarray}
$$
The slow zig-zag path of steepest descent is a real problem. It shows the zig-zag problem very clearly when $\lambda _{min}/\lambda _{max}=m/M$ is small. We have to improve it.

# 1. Momentum added

We add momentum with coefficient $\beta$ to the gradient. $z_k$ is the search direction, the direction $z_k$ of the new step remembers the previous direction $z_{k-1}$.
$$
x_{k+1}=x_k-sz_k \\
z_k=\nabla f(x_k)+\beta z_{k-1}
$$
The step to $x_{k+1}$ in equation involves $z_{k-1}$. Momentum has turned a one-step method (gradient descent) into a two-step method. To get back to one step, we have to rewrite for the state at time $k+1$ :
$$
\begin{eqnarray}
x_{k+1}\  &=&x_k-sz_k \\
z_{k+1}-\nabla f(x_{k+1})&=& \ \ \ \ \ \ \ \ \ \beta z_k
\end{eqnarray}
$$
When $f(x)=\frac{1}{2}x^TSx$ is quadratic, its gradient $\nabla f = Sx$ is linear. We rewrite the equations:
$$
\begin{eqnarray}
x_{k+1}&=&x_k-sz_k \\
z_{k+1}-Sx_{k+1}&=& \ \ \ \ \ \ \ \ \ \beta z_k
\end{eqnarray}
$$
We have a constant coefficient problem at every step, the $x$, $z$ variables is multiplied by a matrix.
$$
\begin{bmatrix}
{1}&{0}\\
{-S}&{1}
\end{bmatrix}
\begin{bmatrix}
{x}\\{z}
\end{bmatrix}_{k+1}=
\begin{bmatrix}
{1}&{-s}\\
{0}&{\beta}
\end{bmatrix}
\begin{bmatrix}
{x}\\{z}
\end{bmatrix}_{k}
$$
We track each eigenvector of $S$. Suppose $Sq=\lambda q$ and $x_k=c_k q$ and $z_k=d_kq$ and $\nabla f_k=Sx_k=\lambda c_k q$. Then the equation connects the coefficient $c_k$ and $d_k$ to $c_{k+1}$ and $d_{k+1}$.
$$
\begin{bmatrix}
{1}&{0}\\
{-\lambda}&{1}
\end{bmatrix}
\begin{bmatrix}
{c_{k+1}}\\
{d_{k+1}}
\end{bmatrix}=
\begin{bmatrix}
{1}&{-s}\\
{0}&{\beta}
\end{bmatrix}
\begin{bmatrix}
{c_{k}}\\
{d_{k}}
\end{bmatrix}
$$
Finally we invert the first matrix to see each descent step clearly:
$$
\begin{bmatrix}
{c_{k+1}}\\
{d_{k+1}}
\end{bmatrix}=
\begin{bmatrix}
{1}&{0}\\
{\lambda}&{1}
\end{bmatrix}
\begin{bmatrix}
{1}&{-s}\\
{0}&{\beta}
\end{bmatrix}
\begin{bmatrix}
{c_{k}}\\
{d_{k}}
\end{bmatrix}=
\begin{bmatrix}
{1}&{-s}\\
{\lambda}&{\beta-\lambda s}
\end{bmatrix}
\begin{bmatrix}
{c_{k}}\\
{d_{k}}
\end{bmatrix}=
R\begin{bmatrix}
{c_{k}}\\
{d_{k}}
\end{bmatrix}
$$
After $k$ steps the starting vector is multiplied by $R^k$. For fast convergence to zero (which is the minimum of $f=\frac{1}{2}x^TSx$) we want both eigenvalues $e_1$ and $e_2$ of $R$ to be as small as possible. Clearly those eigenvalues of $R$ depend on the eigenvalue $\lambda$ of $S$. Our problem is : **Choose $s$ and $\beta$ to minimize $max(|e_1|,|e_2|)$** for $m \le \lambda \le M$.

It seems a miracle that this problem has a beautiful solution. The optimal $s$ and $\beta$ are
$$
s=(\frac{2}{\sqrt{M}+\sqrt{m}})^2 \\
\beta=(\frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}})^2
$$
For our example $f(x,y)=\frac{1}{2}(x^2+by^2)$, $M=1$ and $m=b$ :
$$
s=(\frac{2}{1+\sqrt{b}})^2 \\
\beta=(\frac{1-\sqrt{b}}{1+\sqrt{b}})^2
$$
These choices of $s$ and $\beta$ give a convergence rate that looks like the rate for ordinary steepest descent. But there is a crucial difference : **b is replaced by $\sqrt{b}$.**

Suppose $b=1/100$, then $\sqrt{b}=1/10$, the convergence factors of steepest descent and momentum are $(\frac{.99}{1.01})^2=.96$ and $(\frac{.9}{1.1})^2=.67$, it's a lot smaller than the ordinary descent.

## 2. Nesterov Acceleration

$$
\begin{eqnarray}
x_{k+1}&=&y_k-s\nabla f(y_k)\\
y_{k+1}&=&x_{k+1}+\beta (x_{k+1}-x_k)
\end{eqnarray}
$$

Accelerated descent involves three parameters $s$, $\beta$, $\gamma$  :
$$
x_{k+1}=x_k+\beta (x_k-x_{k-1})-s\nabla f(x_k+\gamma (x_k-x_{k-1}))
$$


