# 12.Computing Eigenvalues and Singular Values

## Eigenvalues

QR factorize

Suppose $A$ is the matrix we want to get singular valuesã€‚
$$
A_0=Q_0R_0
$$
matrix $Q_0$ and $R_0$ multiplies in  a reverse order we get $A_1$.
$$
A_1 = R_0Q_0
$$
$A_1$ and $A_0$ have the same eigenvalues for they are **similar**.

Repeat that process to make the numbers off-diagonal get smaller.

<img src="./12.Computing Eigenvalues and Singular Values/1.png" style="zoom:20%;" />

When the number there  is very small $\epsilon$, the numbers are not going to change the eigenvalues too much and we can get the eigenvalues of matrix $A$ by the diagonal of the $A_n$. 

The lowest corner one of diagonal $a_{mm}^n$ is accurate first.

e.g.
$$
A_0 = \left[\begin{matrix} cos\theta & ? \\ sin\theta & ? \\ \end{matrix}\right] \\
A_1 = \left[\begin{matrix} ? & ? \\ (sin\theta)^3 & ? \\ \end{matrix}\right]
$$
the off-diagonal gets cubes and next step will be 9 times.

**Imporve**

*Introduce shifts.*

Instead of $A_0$,  take $A_0-sI$, which shift is some multiple of the idenity.And eigenvalues varies with $s$.
$$
A_0-sI = Q_0R_0 \\
R_0Q_0+sI = A_1
$$
We hope $A_0$ and $A_0$ are still similar.:
$$
\begin{align}
A_1 & = R_0Q_0+sI = R_0(A-sI)R_0^{-1} + sI \\
&=R_0A_0R_0^{-1}
\end{align}
$$
And this makes convergence faster.

**further improve**

*reduce the time cost on factorize*

if $A$ has lots of zeros , some steps can be skip.

1. reduce $A$ to **Hessenberg form** which has extra diagonal off the diagonal.
2. $QR$ with Shifts.

The method can be applied to symmetric matrix easily. The Sysmmetrix Hessenberg matrix is tridiagonal matrix.

For sysmmetric matrix $S$, we can use orthogonal matrix $Q$ to make the matrix tridiagonal.:
$$
QSQ^T = QSQ^{-1} = tridiagonal\ matrix
$$
They have the same eigenvalues with general matrix $A$.

## connected to the SVD

What operation will not change the sigular value of the matrix:
$$
A=U\Sigma V^T
$$
The answer is to mutiply with orthogonal matrix $Q$.

> The multiplication result of two orthogonal matrices is still an orthogonal matrix.
> $$
> (QU)^{-1} = U^{-1}Q^{-1}= U^TQ^T = (QU)^T
> $$

So we can mutiply any orthogonal matrix on the left and right but not change the singular values:
$$
(Q_1U)\Sigma (V^TQ_2^T)
$$
This allows us to do more operations that we can reduce the zero to  **bidiagonal matrix.** And then doing QR factorization.

It is same with redcue $A^TA$ to **tridiagonal.**

The method gets gaint success on big matrix.

## What about much bigger matrix?

Krylov space:
$$
b, Ab, A^2b ,\cdots, A^{99}b
$$
restrict the matrix to the 100 dimension space and hopes that eigenvalues is virtually in that space.

So we have $100\times 100$ problem to approximate the first 100 eigenvalues of $A$.