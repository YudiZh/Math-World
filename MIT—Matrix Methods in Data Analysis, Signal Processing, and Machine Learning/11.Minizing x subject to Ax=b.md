# 11.Minizing ||x|| subject to Ax=b

## Gram-Schmidt

1. Standard way $A\rightarrow QR$  (18.06)
   $$
   A=\left[\begin{matrix} a_1 & a_2 & \cdots &a_n\end{matrix}\right] \rightarrow Q = [\begin{matrix} q_1 & q_2 & \cdots & q_n  \end{matrix}]
   $$
   Gram-Schmidt get **orthogonal** basis of the space which is compsed of the nearly depedent vectors.

   ***how to get $R$?***
   $$
   A = QR
   $$
   For Q is orthogonal：
   $$
   R=Q^TA = \left[\begin{matrix} q_1^T \\ q_2^T \\ \vdots \\ q_n^T \end{matrix}\right] \left[\begin{matrix}a_1 & a_2 & \cdots & a_n \end{matrix}\right]
   $$
   ***How to get $Q$?***

   The process has two steps :

   1. Correct the direction based on $a_i$
   2. normalize the vector

   $$
   q_1 = \frac{a_1}{||a_1||} \\
   A_2 = a_2 -( a_2^T q_1 )q_1 \\
   q_2 = \frac{A_2}{||A_2||} \\
   A_3 = a_3 - (a_3^Tq_1)q_1 - (a_3^Tq_2)a_2 \\
   q_3 = \frac{A_3}{||A_3||} \\
   \vdots
   $$

2. Column exchanges

   The Standard way of Gram-Schimdt gets the orthogonal vectors in order. However , when the $a_1$ is very near $a_2$, the $A_2$ we get is a very small vector which will causes round-off errors calculating the $q_2$. 

   Now we wants the remaining part $A_i$ to be large enough to avoid the error by permuating the rows to pick the biggest.

   And this also happens on the first vector $a_1$. 

   The process is as follows：

   - if $a_1$ is a very tiny vector, we should look for a bigger one as $q_1$. 

   - For $q_2$ , get the biggest remaining part as $A_2$:
     $$
     a_2-(a_2^Tq_1)q_1 \\
     a_3-(a_3^Tq_1)q_1 \\
     a_4-(a_4^Tq_1)q_1 \\
     \cdots \\
     a_n-(a_n^Tq_1)q_1
     $$
     calculating above equations and choose the biggest one as the $A_2$

   - Permute the columns and calculating $A_i$ according to remaining vectors in the previous step.

3. "Kylov Russian Armoldi"

   $Ax=b$ which $A$ is a large sparse matrix.

   multiply the matrix with vector:
   $$
   b, Ab, A(Ab), \cdots, A^{j-1}b
   $$
   Combinations of above vectors give Krylov Space $K$.

   We don't find the solution of $Ax=b$ direclty but find the best solution, the closest solution solution, the least square solution in this Krylov space.

   $x_j$ is the best vector $K$.

   The vectors may be nearly dependent and are not suitable for basis of $K$. So we need to orthogoanlize and normalize them by "Lacos" what they do is just to using Gram-Schimdt.

   The benifit of taking orth-norm basis:
   $$
   x \approx c_1q_1 + c_2 q_2 + \cdots + c_nq_n=[Q][c]
   $$

   - When Q are not orth-norm vectors, the pay-off to get $c$ is $c=Q^{-1}x$
   - When Q are orth-norm vectors, the pay-off to get $c$ is $c=Q^{-1}x= Q^Tx$

   

   

   